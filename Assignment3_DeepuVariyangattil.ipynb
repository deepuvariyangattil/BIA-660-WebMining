{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from reppy.robots import Robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_absolute(url):\n",
    "    \"\"\"Determine whether URL is absolute.\"\"\"\n",
    "    return bool(urlparse(url).netloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")\n",
    "#driver = webdriver.Chrome(chrome_options=options)\n",
    "driver = webdriver.Chrome(executable_path='F:\\Stevens_Masters\\Spring 19\\Web Mining\\Week 3\\chromedriver_win32\\chromedriver.exe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_addresses = []\n",
    "q = queue.Queue()\n",
    "q.put(\"https://www.stevens.edu/\")\n",
    "mydata=[[\"URL\",\"DIRECTORY\"]]\n",
    "myFile = open(\"linkinfo.csv\", 'w',newline='')\n",
    "with myFile:\n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"root\"):\n",
    "    os.mkdir(\"root\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "os.chdir('C://Users//DEEPUAISHU//root')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list=[]\n",
    "for i in range(300000):\n",
    "    #print(\"round {}.format\",i)\n",
    "    url = q.get()\n",
    "    #print(url)\n",
    "    if(url not in url_list):\n",
    "        url_list.append(url)\n",
    "        #print(url)\n",
    "        #finding crawl delay and reducing 4 seconds to it(only for stevens.edu)\n",
    "        robots = Robots.fetch('http://stevens.edu/robots.txt')\n",
    "        crawl_delay=robots.agent('foo').delay\n",
    "        time.sleep(crawl_delay-4)\n",
    "         \n",
    "        driver.get(url)\n",
    "        page_name=\"root\"\n",
    "        #print(driver.page_source)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        o=urlparse(url)\n",
    "        #saving source into a text file\n",
    "        path=s=o.path\n",
    "        if(path==\"/\" or path==\"/#\" ):\n",
    "            path=page_name\n",
    "        else:\n",
    "            path=page_name+path\n",
    "\n",
    "        #print(path)\n",
    "        \n",
    "        page_data=[soup.contents]\n",
    "        way=path\n",
    "        #replacing unwanted characters while creating file name\n",
    "        p=way.replace(\"/\",\"_\")\n",
    "        p=p.replace(\"<\",\"\")\n",
    "        p=p.replace(\">\",\"\")\n",
    "        p=p.replace(\"?\",\"\")\n",
    "        p=p.replace(\":\",\"\")\n",
    "        p=p.replace(\"*\",\"\")\n",
    "        p=p.replace(\"\\t\",\" \")\n",
    "        \n",
    "        with open(p+\".txt\",'w+',encoding=\"utf-8\") as f:\n",
    "            for e in page_data:\n",
    "                f.write(str(e))\n",
    "                f.close\n",
    "        #saving link and path to a csv file    \n",
    "        mydata=[[url,path]]\n",
    "        myFile = open(\"linkinfo.csv\", 'a',newline='',encoding=\"utf-8\")\n",
    "        with myFile:\n",
    "            writer = csv.writer(myFile)\n",
    "            writer.writerows(mydata)\n",
    "            #finding email address in a page(professor program logic)\n",
    "        email_addresses += re.findall(\"\\S+@stevens.edu\", soup.get_text())\n",
    "        email_addresses = list(set(email_addresses))\n",
    "        links = soup.find_all('a')\n",
    "        for link in links:\n",
    "            u = link.get('href')\n",
    "            if not is_absolute(u):\n",
    "                u = urljoin(url, u)\n",
    "                #print(u)\n",
    "            if \"stevens.edu\" in u:\n",
    "                q.put(u)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"email.txt\", \"w+\") as f:\n",
    "    for e in email_addresses:\n",
    "        f.write(e + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
